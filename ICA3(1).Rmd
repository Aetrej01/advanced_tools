---
title: "BLP Assignment"
author: "Alice Trejo"
date: "MSBA Data Analytics III"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

Propsensity Score Matching

For this problem, you will analyze the data from:

  Chirstopher Blattman and J Annan. 2010. "The consequences of child soldiering" _Review of Economics and Statistics_ 92 (4):882-898

The data are from a panel survey of male youth in war-afflicted regions of Uganda. The authors want to estimate the impact of forced military service on various outcomes. They focus on Uganda because there were a significant number of abductions of young men into Lord's Resistance Army.

Blattman and Annan describe the abductions as follows:

Abduction was large-scale and seemingly indiscriminate; 60,000 to 80,000 youth are estimated to have been abducted and more than a quarter of males currently aged 14 to 30 in our study region were abducted for at least two weeks. Most were abducted after 1996 and from one of the Acholi districts of Gulu, Kitgum, and Pader. 

Youth were typically taken by roving groups of 10 to 20 rebels during night raids on rural homes. Adolescent males appear to have been the most pliable, reliable and effective forced recruits, and so were disproportionately targeted by the LRA. Youth under age 11 and over 24 tended to be avoided and had a high probability of immediate release. Lengths of abduction ranged from a day to ten years, averaging 8.9 months in our sample. Youth who failed toe escape were trained as fighters and, after a few months, received a gun. Two thirds of abductees were forced to perpetrate a crime or violence. A third eventually became fighters and a fifth were forced to murder soldiers, civilians, or even family members in order to bind them to the group, to reduce their fear of killing, and to discourage disobedience.

In this problem we will look at the effect of abduction on _educ_ (years of education). The _abd_ variable is the treatment in this case. Note that _educ, distress, and logwage_ are all outcomes/post-treatment variables.

Variables | Description
----------|----------------------------------------
 abd | abducted by the LRA (the treatment)
 c_ach - c_pal | Location indicators (each abbreviation corresponds to a subdistrict; i.e. ach = Acholibur, etc.)
 age | age in years
 fthr_ed | father's education (years)
 mthr_ed | mother's education (years)
 orphan96 | indicator if parent's died before 1996
 hh_fthr_frm | indicator if father is a farmer
 hh_size96 | household size in 1996
 educ | years of education
 distress | index of emotional distress (0-15)
 logwage | log of average daily wage earned in last 4 weeks
 
 1. Calculate the naive Average Treatment Effect (ATE) of abduction on education (educ), distress (distress), and wages (logwage). Do this by running three separate regressions. 
 
```{r eval=TRUE, warning=FALSE, message=FALSE}
blattman <- read.csv("~/UofL/MSBA/Advanced Tools/ICA 3/blattman.csv")
reg1 <-lm(educ ~ abd, data =blattman)
reg2 <-lm(distress ~ abd, data = blattman)
reg3 <-lm(log.wage ~ abd, data = blattman)
library(modelsummary)
modelsummary(list("Education"=reg1, "Distress"=reg2, "Log Wage"=reg3), stars = TRUE)
```
 
 
 2. Use a parametric model (Probit/Logit) to calculate the propensity scores for each person in the data to be abducted. Include whatever covariates or functions of covariates you think may be important.
 
```{r eval=TRUE}
reg4 <- glm(abd ~ age+fthr_ed+mthr_ed+orphan96+hh_fthr_frm+hh_size96, family = binomial, data = blattman)
modelsummary(list('Logistic'=reg4), stars = TRUE)
```
 
 
 3. Use optimal match over the whole data set to estimate the ATE using propensity score matching. Do this for all three dependent variables. 
 
```{r eval=TRUE}
#install.packages('MatchIt')
library(MatchIt)
m.nn<-matchit(abd ~ age+fthr_ed+mthr_ed+orphan96+hh_fthr_frm+hh_size96, data = blattman, ratio=1, method ="optimal")
nn.match<-match.data(m.nn)

reg5 <- lm(educ ~ abd, data =nn.match )
reg6 <- lm(distress ~ abd, data =nn.match )
reg7 <- lm(log.wage ~ abd, data =nn.match )

modelsummary(list("Education"=reg5,"Distress"=reg6,"Log Wages"=reg7),stars = TRUE, coef_rename = coef_rename, title = "Propensity Score Matching Results")
```
 
 
 4. Use the cobalt package to make a "Love plot". You can find information of the cobalt package [here](https://cran.r-project.org/web/packages/cobalt/vignettes/cobalt_A0_basic_use.html)
 
```{r eval=TRUE}
#install.packages('cobalt')
library(cobalt)
b1<-bal.tab(abd ~ hh_size96 + hh_fthr_frm+orphan96+mthr_ed+fthr_ed+age,data=blattman,int = TRUE)
v1<-var.names(b1, type = "vec", minimal = TRUE)
v1["hh_size96"]<-"Household Size in 1996"
v1["hh_fthr_frm"]<-"Father was Farmer"
v1["orphan96"]<-"Orphaned in 1996"
v1["mthr_ed"]<-"Mother's Education"
v1["fthr_ed"]<-"Father's Education"
v1["age"]<-"Age"

love.plot(m.nn,binary = "std", var.names=v1) #use v1 for your variable names
```
 

Problem Set: BLP Methodology
In this problem you will perform demand estimation using market level data. Run the following code in R
```{r eval=TRUE, warning=FALSE, message=FALSE}
#install.packages("BLPestimatoR")
library(BLPestimatoR)
data(productData_cereal)
```
A table of market shares, prices, and characteristics of the top-selling brands of cereal in 1992 across several markets is now available in your environment. The data are aggregated from household-level scanner data (collected at supermarket checkout counters).
We observe the following variables 

price = price paid for the cereal 
const = just a column of 1â€™s that you can ignore. 
sugar = how much sugar is in the cereal 
mushy = how mushy the cereal becomes with milk.
share = market share of the cereal in that particular market. This number is between 0 and 1. 
cdid = tells you which market you are in. 
product_id = tells you which cereal is captured. 
IV1-IV20 = 20 constructed instrumental variables.

1. Find the market share of the outside good in every market. That is, sum all of the shares across all of the cereals for each market. You will notice that this number is less than 1. The market share of the outside option is equal to 1 - total cereal market share in each market. (Hint: you can use the aggregate to sum up the cereal shares by market)

```{r eval=TRUE}
# We can use the function ave(variable, grouping variable, FUN = function(x) 1-sum(x))

productData_cereal$outside_share <- ave(productData_cereal$share, productData_cereal$cdid, FUN = function(x) 1-sum(x))
# this will create your new depedent variable (i.e. log(sj)-log(so))
productData_cereal$y <- log(productData_cereal$share)-log(productData_cereal$outside_share)

```


2. Estimate the share regression using mushy, sugar, and price as explanatory variables using OLS

```{r eval=TRUE}
#install.packages('fixest')
library(fixest)
## In this section we are just going to run OLS on the linear demand curve
blp.reg.1<-lm(y~mushy+sugar+price, data=productData_cereal)
blp.reg.2 <-feols(y~mushy+sugar+price | cdid, data=productData_cereal) # Include market fixed effects
```

3. 2SLS: Use the instrumental variables IV1 - IV10 to instrument for price
```{r eval=TRUE}
blp.reg.3 <- feols(y~mushy+sugar| cdid|  price ~ IV1+IV2+IV3+IV4+IV5+IV6+IV7+IV8+IV9+IV10, data=productData_cereal)
modelsummary(list("OLS"=blp.reg.1, "FE"=blp.reg.2, "IV"=blp.reg.3),stars = TRUE)
```

4. 2SLS: perform the first stage F-stat test to judge the strength of your instruments and the second stage sargent test to see if these instruments are independent of the error term.

```{r eval=TRUE}
# Hint use the summary function with object bls.reg.3
#Answer: The p-value(0.00) is less than 0.05, this means that we can reject the null hypothesis that these instruments are valid. Suggesting that these instruments are not independent of the error term. 
summary(blp.reg.3)

```


5. 2SLS: can you use a smaller set of instruments to get a better result? If so, then what instruments did you include? Report your results including the first stage F-stats and the overidentification test.

Hint: You will need to run the first stage regression and identify which instruments are significant. Try using only the significant instruments.

```{r eval=TRUE}
productData_cereal$resid <- blp.reg.3$residuals 
test1<- feols(resid~ mushy+sugar+IV1+IV2+IV3+IV4+IV5+IV6+IV7+IV8+IV9+IV10|cdid, data=productData_cereal)
summary(test1)
#Answer: We identified the instrumental variables: IV3, IV7, and IV8 to be correlated with the error term. Then, we excluded these IV's and ran a 2SLS regression using only the significant instruments to get a better result. 

cr_reg4 <- c("mushy" = "Mushy", "sugar" = "Sugar","price"="Price", "fit_price"="Fit Price")

blp.reg.4 <- feols(y~mushy+sugar | cdid | price ~ IV1+IV2+IV4+IV5+IV6+IV9+IV10, data=productData_cereal)
modelsummary(list('Ordinary Least Squares'=blp.reg.1, 'Fixed Effects'=blp.reg.2, 'Instrumental Variables'=blp.reg.3, 'IV-restricted'=blp.reg.4), stars=TRUE, coef_rename = cr_reg4)
```

